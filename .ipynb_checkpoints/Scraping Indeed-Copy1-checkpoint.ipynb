{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as ur\n",
    "import re\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#The first page with search results\n",
    "\n",
    "#url_base = \"https://www.indeed.com/jobs?q=data+science&\"\n",
    "url_base = \"https://www.indeed.com/jobs?q=data+science&sort=date\"\n",
    "\n",
    "\n",
    "#response = ur.urlopen(url_base+'start=' +str(n))\n",
    "response = ur.urlopen(url_base)\n",
    "html_doc = response.read()\n",
    "    \n",
    "#print(pgstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_results  = soup.find(id=\"searchCount\").get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14286\n"
     ]
    }
   ],
   "source": [
    "# Number of search results \n",
    "\n",
    "total_results  = soup.find(id=\"searchCount\").get_text()\n",
    "\n",
    "\n",
    "number_of_jobs = total_results[total_results.index(\"of\")+2: total_results.index(\"jobs\")].strip()\n",
    "last_job_number = int(number_of_jobs[:number_of_jobs.index(\",\")]+number_of_jobs[number_of_jobs.index(\",\")+1:])\n",
    "print(last_job_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next 10 from # 10\n",
      "next 10 from # 20\n",
      "next 10 from # 30\n",
      "next 10 from # 40\n",
      "next 10 from # 50\n",
      "next 10 from # 60\n",
      "next 10 from # 70\n",
      "next 10 from # 80\n",
      "next 10 from # 90\n",
      "next 10 from # 100\n",
      "next 10 from # 110\n",
      "next 10 from # 120\n",
      "next 10 from # 130\n",
      "next 10 from # 140\n",
      "next 10 from # 150\n",
      "next 10 from # 160\n",
      "next 10 from # 170\n",
      "next 10 from # 180\n",
      "next 10 from # 190\n",
      "next 10 from # 200\n",
      "next 10 from # 210\n",
      "next 10 from # 220\n",
      "next 10 from # 230\n",
      "next 10 from # 240\n",
      "next 10 from # 250\n",
      "next 10 from # 260\n",
      "next 10 from # 270\n",
      "next 10 from # 280\n",
      "next 10 from # 290\n",
      "next 10 from # 300\n",
      "next 10 from # 310\n",
      "next 10 from # 320\n",
      "next 10 from # 330\n",
      "next 10 from # 340\n",
      "next 10 from # 350\n",
      "next 10 from # 360\n",
      "next 10 from # 370\n",
      "next 10 from # 380\n",
      "next 10 from # 390\n",
      "next 10 from # 400\n",
      "next 10 from # 410\n",
      "next 10 from # 420\n",
      "next 10 from # 430\n",
      "next 10 from # 440\n",
      "next 10 from # 450\n",
      "next 10 from # 460\n",
      "next 10 from # 470\n",
      "next 10 from # 480\n",
      "next 10 from # 490\n",
      "next 10 from # 500\n",
      "next 10 from # 510\n",
      "next 10 from # 520\n",
      "next 10 from # 530\n",
      "next 10 from # 540\n",
      "next 10 from # 550\n",
      "next 10 from # 560\n",
      "next 10 from # 570\n",
      "next 10 from # 580\n",
      "next 10 from # 590\n",
      "next 10 from # 600\n",
      "next 10 from # 610\n",
      "next 10 from # 620\n",
      "next 10 from # 630\n",
      "next 10 from # 640\n",
      "next 10 from # 650\n",
      "next 10 from # 660\n",
      "next 10 from # 670\n",
      "next 10 from # 680\n",
      "next 10 from # 690\n",
      "next 10 from # 700\n",
      "next 10 from # 710\n",
      "next 10 from # 720\n",
      "next 10 from # 730\n",
      "next 10 from # 740\n",
      "next 10 from # 750\n",
      "next 10 from # 760\n",
      "next 10 from # 770\n",
      "next 10 from # 780\n",
      "next 10 from # 790\n",
      "next 10 from # 800\n",
      "next 10 from # 810\n",
      "next 10 from # 820\n",
      "next 10 from # 830\n",
      "next 10 from # 840\n",
      "next 10 from # 850\n",
      "next 10 from # 860\n",
      "next 10 from # 870\n",
      "next 10 from # 880\n",
      "next 10 from # 890\n",
      "next 10 from # 900\n",
      "next 10 from # 910\n",
      "next 10 from # 920\n",
      "next 10 from # 930\n",
      "next 10 from # 940\n",
      "next 10 from # 950\n",
      "next 10 from # 960\n",
      "next 10 from # 970\n",
      "next 10 from # 980\n",
      "next 10 from # 990\n"
     ]
    }
   ],
   "source": [
    "frame = []\n",
    "\n",
    "jobs_per_page = 10\n",
    "\n",
    "#for pgno in range(1450, last_job_number, jobs_per_page):\n",
    "for pgno in range(0, 1000, jobs_per_page):\n",
    "    if pgno == 0:\n",
    "        try:\n",
    "            response = ur.urlopen(url_base)\n",
    "            html_doc = response.read()    \n",
    "        except:\n",
    "            break;\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            response = ur.urlopen(url_base+'start=' +str(pgno))\n",
    "            html_doc = response.read()\n",
    "        except:\n",
    "            break;\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        print('next 10 from #', pgno)\n",
    "        \n",
    "    for job in soup.find_all(class_='result'):\n",
    "        link = job.find(class_=\"turnstileLink\")\n",
    "        \n",
    "        try:\n",
    "            jt = link.get('title')\n",
    "        except:\n",
    "            jt = \"\"\n",
    "            \n",
    "        try:\n",
    "            comp = job.find(class_='company').get_text().strip()\n",
    "        except:\n",
    "            comp = \"\"\n",
    "            \n",
    "        try:\n",
    "            date = job.find(class_='date').get_text().strip()\n",
    "        except:\n",
    "            date = \"\"\n",
    "            \n",
    "        try:\n",
    "            loc = job.find(class_='location').get_text().strip()\n",
    "        except:\n",
    "            loc = \"\"\n",
    "\n",
    "        job_url = \"http://www.indeed.com\"+link.get('href')\n",
    "        try:\n",
    "            html_doc = ur.urlopen(job_url).read().decode('utf-8')\n",
    "        except:\n",
    "            html_doc = \"\"\n",
    "            continue;\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        \n",
    "        #print(job_url)\n",
    "        #print(comp)\n",
    "        \n",
    "        try:\n",
    "            job_ad = soup.find(class_ ='jobsearch-JobComponent-description').get_text().strip()\n",
    "        except:\n",
    "            job_ad = \"\"\n",
    "              \n",
    "        #print(job_ad)\n",
    "        try:\n",
    "            salary = soup.find(class_ ='jobsearch-JobMetadataHeader-item').get_text().strip()\n",
    "        except:\n",
    "            salary = \"\"\n",
    "            \n",
    "        #print(salary)\n",
    "        \n",
    "        frame.append({'job_title': jt, 'job_url': job_url, 'company':comp, 'date': date, \n",
    "                      'location':loc, \n",
    "                      'salary': salary, \n",
    "                      'job_description': job_ad})\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(frame)\n",
    "df.shape\n",
    "#df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.job_url.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['job_description'].apply(len)\n",
    "df.tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd \"/Users/Tetiana/Desktop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"data_science_jobs_part2.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data_science_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = df2.head()\n",
    "fd2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df[,'job_description']\n",
    "\n",
    "sample.to_csv(\"data_science_jobs_sample.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   row = ( \"\\\"\"+ jt +\"\\\", \" + \n",
    "               \"\\\"\"+ job_url +\"\\\", \" + \n",
    "               \"\\\"\"+ comp +\"\\\", \"+ \n",
    "               \"\\\"\"+ date +\"\\\", \"+ \n",
    "               \"\\\"\"+ loc +\"\\\", \"+ \n",
    "               \"\\\"\"+ salary +\"\\\", \"+\n",
    "               \"\\\"\"+ job_ad +\"\\\"\"+\n",
    "               \"\\n\")\n",
    "               \n",
    "               \n",
    "            row = (\"\\\"\"+ jt +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ job_url +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ comp +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ date +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ loc +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ salary +\"\\\"\" + \",\" +\n",
    "               \"\\\"\"+ job_ad +\"\\\"\" + \n",
    "               \"\\n\")\n",
    "        print('-------------------------',\n",
    "              '---------------------------')   \n",
    "        #if len(row.split(\"\\\",\\\"\"))==7:\n",
    "         #   print(row)\n",
    "         #   f_file.write(row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data_scientist_jobs.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(row.split(\"\\t\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_scientist_jobs.csv\",  error_bad_lines=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "with open(\"data_scientist_jobs.csv\", \"r\") as ins:\n",
    "    array = []\n",
    "    for line in ins:\n",
    "        try:\n",
    "            data.append(line)\n",
    "        except:\n",
    "            continue;\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "#data = pd.read_csv(\"data_scientist_jobs.csv\")\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(time.strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k =  (time.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
